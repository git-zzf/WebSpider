# 单元小结

## Requests库的7个方法

| 方法                | 说明                                           |
| ------------------- | ---------------------------------------------- |
| requests.requests() | 构造一个请求，支撑以下各方法的基础方法         |
| requests.get()      | 获取HTML网页的主要方法，对应与HTTP的GET        |
| requests.head()     | 获取HTML网页头信息的方法，对应于HTTP的HEAD     |
| requests.post()     | 向HTML网页提交POST请求的方法，对应于HTTP的POST |
| requests.put()      | 向HTML网页提交PUT请求的方法，对应于HTTP的PUT   |
| requests.patch()    | 向HTML网页提交局部修改请求，对应于HTTP的PATCH  |
| requests.delete()   | 向HTML页面提交删除请求，对应于HTTP的DELETE     |

事实上，由于网络安全的限制，我们很难向一个URL去发起post()，put()，patch()和delete()的请求。而request()方法又是一个基础方法。

因此，真正使用Requests库，如果作为爬虫功能来讲，最常使用的功能就是get()。对于某些特别大的链接，我们使用head()方法来获得它的资源概要。

因此，对于Requests库来讲，重点掌握get()和head()两个方法就可以。

## 爬取网页的通用代码框架

```python
def getHTMLText(url):
    try:
        r = requests.get(url, timeout = 30)
        r.raise_for_status() #如果状态不是200，引发HTTPError异常
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return "产生异常"
```

通过try和except来实现的代码组织。

***网络连接有风险，异常处理很重要***

一定要用try和except方式来保证连接它的异常能够被有效处理。

在通用代码框架中，一行核心代码是：

`r.raise_for_status()`

也就是response对象的raise_for_status()函数，这个方法的作用是：如果返回的对象，它的状态码不是200，就是说信息没有正确获得，它将产生一次异常。所以，except就能捕获到所有网络连接错误时的异常。