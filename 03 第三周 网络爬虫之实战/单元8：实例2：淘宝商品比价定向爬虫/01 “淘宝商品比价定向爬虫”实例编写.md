# “淘宝商品比价定向爬虫”实例编写

思路：四个函数

## 主函数:`main()`

主函数内需要给出要爬取的网页链接，商品名称，查找深度（翻多少页），调用其它3个函数。

```python
def main():
    goods = '书包'
    depth = 2
    start_url = 'https://s.taobao.com/search?q=' + goods
    infoList = []
    for i in range(depth):
        try:
            url = start_url + '&s=' + str(44*i)
            html = getHTMLText(url)
            parsePage(inforList, html)
        except:
            continue
   	printGoodsList(infoList)

main()
```





## 获取HTML的函数：`getHTMLText(url)`

调用requests库，使用try...except框架，使用`r.encoding = r.apparent_encoding`进行解码。得到的网页`r.text`存于变量HTML。

```python
import requests
kv = {'user-agent':'Mozilla/5.0'}
try:
    r = requets.get(url, timeout = 30, headers = kv)
    r.raise_for_status()
    r.encoding = r.apparent_encoding
    return r.text
except:
    return ""

```





## 分析页面的函数：`parsePage(ilt, html)`

调用Re正则表达式库，解析网页中的价格以及商品名称的信息，使用正则表达式匹配字符串。

由于这个网页不需要通过检索标签的方式解析，所以不用调用BeautifulSoup库，用列表保存结果。

因为是分析字符串，所以可以使用`eval()`函数。

+ try...except

+ `re.findall()`，搜索字符串，以列表类型返回全部能匹配的子串

+ 正则表达式语法：

  + | 操作符 | 说明                           | 实例                                    |
    | ------ | ------------------------------ | --------------------------------------- |
    | .      | 表示任何单个字符               |                                         |
    | []     | 字符集，对单个字符给出取值范围 | [abc]表示a、b、c、[a-z]表示a到z单个字符 |
    | *      | 前一个字符0次或无限次扩展      | abc*表示ab、abc、abcc、abccc等          |
    | ?      | 前一个字符0次或1次扩展         | abc？表示ab、abc                        |
    | \d     | 数字，等价于[0-9]              |                                         |

分析网页，价格被保存在`"view_price":"149.00"`中，名称被保存在`"raw_title":"书包"`中

获取商品价格，可以使用`re.findall()`函数匹配字符串获得：

```python
pltre = r'"view_price":"[\d.]*"' 
plt = re.findall(pltre, html)
```

获取商品名称同理：

```python
tltre = r'"raw_title":".*?"'
tlt = re.findall(tltre, html)
```

保存的时候需要只保存价格部分，和商品名称部分，所以还要进一步提取（现在得到的是`view_price:价格`），可以使用`eval()`函数对每个商品分别处理。

```python
price = eval(plt[i].split(':')[1])
```

在这里`plt[i].split(':')`会得到`["view_price", "价格"]`，选择第二个元素就是`"价格"`，使用`eval()`函数去掉双引号，转化成数字赋值给变量`price`

`re.findall()`函数返回的是一个列表，包含了所有商品，列表长度就是商品数量，使用for循环对每个商品进行提取，并用ilt列表储存结果

```python
for i in range(len(plt)):
    price = eval(plt[i].split(':')[1])
    title = eval(tlt[i].split(':')[1])
    ilt.append([price, title])
```

将全部代码放在`try...except`框架中，因为可能会遇到问题，比如`re.findall()`函数找不到符合要求的字符串，或者当使用`eval()`去掉双引号也可以出现错误，这时候就需要使用`try...except`框架，保证程序运行：

```python
try:
    plt = re.findall(r'"view_price":"[\d.]*"', html)
    tlt = re.findall(r'"raw_title":".*?"', html)
    for i in range(len(plt)):
    price = eval(plt[i].split(':')[1])
    title = eval(tlt[i].split(':')[1])
    ilt.append([price, title])
except:
    print("")
```





## 打印信息的函数：`printGoodsList(ilt)`

把列表中的信息打印成3列的表格，序号，价格，名称。

设计打印模板：

```python
tplt = "{:4}\t{:8}\t{16}"
```

打印输出信息的表头：

```python
print(tplt.format("序号", "价格", "商品名称"))
```

输出所有的信息：

```python
count = 0
for g in ilt:
    count = count + 1
    print(tplt.format(count, g[0], g[1]))
```





## 全代码

```python
import requests
import re


def getHTMLText(url):
    try:
        header = {
            'user-agent': 'Mozilla/5.0',
            'cookie': '自己的cookie',
        }
        r = requests.get(url, timeout=30, headers=header)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        print("爬取失败")
        return ""


def parsePage(ilt, html):
    try:
        plt = re.findall(r'"view_price":"[\d.]*"', html)
        tlt = re.findall(r'"raw_title":".*?"', html)
        for i in range(len(plt)):
            price = eval(plt[i].split(':')[1])
            title = eval(tlt[i].split(':')[1])
            ilt.append([price, title])
    except:
        print("解析出错")


def printGoodsList(ilt):
    tplt = "{:4}\t{:8}\t{:16}"
    print(tplt.format("序号", "价格", "商品名称"))
    count = 0
    for g in ilt:
        count = count + 1
        print(tplt.format(count, g[0], g[1]))


def main():
    goods = '铅笔'
    depth = 1
    start_url = 'https://s.taobao.com/search?q=' + goods
    infoList = []
    for i in range(depth):
        try:
            url = start_url + '&S=' + str(44 * i)
            html = getHTMLText(url)
            parsePage(infoList, html)
        except:
            continue
    printGoodsList(infoList)


main()

```





## 查找cookie的办法

根据`https://blog.csdn.net/Guanhai1617/article/details/104120581`的方法得到cookie

1. 登录淘宝，进入搜索页，F12
2. 选择Network，刷新一下，找到最上方以search？开头的文件，右键
3. 选择copy，copy as cURL（bash）
4. 在https://curl.trillworks.com/，将上一步复制的内容粘贴到curl command窗口
5. 复制右侧的headers内容，在程序中用以变量header保存，作为参数传给requests.get(url，headers=header)





## 错误总结

1. `infoList`拼写成`inforList`拼写错误
2. `tplt = "{:4}\t{:8}\t{:16}"`的`{:16}`忘记加冒号
3. 从Typora复制到IDLE缩进出错