[TOC]



# "股票数据定向爬虫"实例介绍

## 功能描述

目标：获取上交所和深交所所有股票的名称和交易信息，交易信息包括当前的交易价格，上一日的成交价等交易数据

输出：保存到文件中

技术路线：requests-bs4-re

选取网站：新浪股票，百度股票（已关闭），东方财富网

选取原则：股票信息应该静态存在候选网站的HTML页面中，非js代码生成的。因为有些网站显示的股票数据是写在代码中是由后台服务器将数据添到代码中后，由前浏览器来解析。还有一类数据是由前台的浏览器使用JavaScript脚本获得，对于这种类型的网站，目前的技术路线无法实现。还要确保没有Robots协议的限制。

选取方法：浏览器F12，源代码查看

选取心态：不要纠结于某个网站，多找信息源尝试





步骤1：东方财富网获取股票列表：`http://quote.eastmoney.com/stock_list.html`

步骤2：根据股票列表到百度股票获取个股信息

步骤3：将结果保存到文件



****

# "股票数据定向爬虫"实例编写

## 调库

```python
import requests
from bs4 import BeautifulSoup
import traceback
import re
```

****

## 获得URL页面

```python
def getHTMLText(url):
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        print('爬取失败')
        return ""
```

****

## 获得股票的信息列表

```python
def getStockList(lst, stockURL):
    html = getHTMLText(stockURL)
    soup = BeautifulSoup(html, 'html.parser')
    a = soup.find_all('a')
    for i in a:
        try:
            href = i.attrs['href']
            lst.append(re.findall(r'[s][hz]\d{6}', href)[0])
        except:
            continue
```

### 股票信息列表HTML页面源代码

```html
<div class="qox">
    <div class="space3">
    </div>
    <div class="quotebody">
        <div id="quotesearch">
            <div class="sltito">股票代码查询一览表：<a href="#sh">上海股票</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="#sz">深圳股票</a></div>
            <div class="sltit"><a name="sh"/>上海股票</div>

            <ul>
                <li><a target="_blank" href="http://quote.eastmoney.com/sh201000.html">R003(201000)</a></li>

                <li><a target="_blank" href="http://quote.eastmoney.com/sh201001.html">R007(201001)</a></li>

                <li><a target="_blank" href="http://quote.eastmoney.com/sh201002.html">R014(201002)</a></li>
            </ul>
        </div>
    </div>
</div>    
```

通过分析源代码，得知，我们需要的股票信息`sh20100`储存在`a`标签的`href`部分，所以先找出网页所有的`a`标签，然后对每一个`a`标签，通过正则表达式找出符合股票格式的编号（sh或sz加6位数字表示），因为有可能找不到符合条件的`a`标签，所以要用`try...except`框架

****

## 获得每一支股票的股票信息，并储存到数据结构

```python
def getStockInfo(lst, stockURL, fpath):
    for stock in lst:
        url = stockURL + stock
        print(url)
        html = getHTMLText(url)
        try:
            if html == "":
                continue
            infoDict = {}
            soup = BeautifulSoup(html, 'html.parser')
            stockInfo = soup.find('table', attrs={'class': 'quote'})
            if isinstance(stockInfo, bs4.element.Tag):
                name = stockInfo.find('a', attrs={'target': '_blank'}).parent
            else:
                continue
            name_text = name.text.split()[0]
            infoDict.update({'股票名称': name_text})
            if isinstance(stockInfo, bs4.element.Tag):
                keyList = stockInfo.find_all('tr')
            else:
                continue
            del keyList[0]

            for i in range(len(keyList)):
                data = keyList[i].find_all('td')
                key1 = unicodedata.normalize('NFKC', data[0].text.split(':')[0])
                val1 = data[0].text.split(':')[1]
                key2 = unicodedata.normalize('NFKC', data[1].text.split(':')[0])
                val2 = data[1].text.split(':')[1]
                infoDict[key1] = val1
                infoDict[key2] = val2
            with open(fpath, 'a', encoding='utf-8') as f:
                f.write(str(infoDict) + '\n')
        except:
            traceback.print_exc()
            continue
```

### 替代网站HTML页面源代码

```html
<table style="width:960px">
    <tr>
		<td id="tdquote" style="width:240px;vertical-align:top ">
            <table style=width:100% class='quote'>
                
                <tr>
                    <td>嘉实港股通A&nbsp;
                        <a href='http://quote.cfi.cn/quote_501311.html' target='_blank'>行情
                        </a>
                    </td>
                    <td>[501311]
                    </td>
                </tr>
                
                <tr>
                    <td>最　新:
                        <span style=color:rgb(255,0,0)>
                            0.90
                        </span>
                    </td>
                    <td>
                        开　盘:0.89
                    </td>
                </tr>
                
                <tr>
                    <td>涨跌幅:
                        <span style=color:rgb(255,0,0)>
                            1.81%
                        </span>
                    </td>
                    <td>涨　跌:
                        <span style=color:rgb(255,0,0)>
                            0.02
                            </td>
                </tr>
                
                <tr>
                    <td>
                        最　高:0.90
                    </td>
                    <td>
                        最　低:0.88
                    </td>
                </tr>
                
                <tr>
                    <td>
                        成交量:11万股
                    </td>
                    <td>
                            换　手:0.00%
                    </td>
                </tr>
                
            </table>
            <img src='http://quote.cfi.cn/drawprice.aspx?style=small&f=9&w=240&h=145&type=min&stockcode=501311' style='margin-top:10px;' border=0>
            <img src='http://quote.cfi.cn/drawprice.aspx?style=small&f=9&w=240&h=145&type=day&stockcode=501311' style='margin-top:10px;' border=0>
            <br/>
        </td>
    </tr>
</table>
        


```

步骤：

1. 对于股票列表中的每一个元素都进行查找

2. 修改页面链接，定位到对应的股票

3. 用getHTMLText找到页面源代码

4. 进行判断，如果页面是空的，就跳过这次循环

5. 采用字典的数据结构储存信息

6. 用BeautifulSoup解析网页，分析标签：

   + 通过分析，发现要找的信息都存在`<table style=width:100% class='quote'>`标签下面，所以先找到这个标签。

+ 股票名称`嘉实港股通A`储存在标签`<td>`下，不过标签`<td>`有很多个，不好定位，所以通过标签`<a href='http://quote.cfi.cn/quote_501311.html' target='_blank'>`的父级标签来定位。当然，也可能`<table>`标签不存在，所以需要用`isinstance()`函数来判断是否存在`<table>`标签。
  + 用`split()`函数分离出股票名称：`嘉实港股通A&nbsp`后面的`&nbsp`表示空格，所以直接用`split()`分离出名称。并把名称储存到字典中，使用`update()`函数可以把另一个键值对添加到一个字典中。
+ 找到所有其它的信息，这些信息储存在`tr`标签下的`td`标签中，通过`find_all()`函数找出所有的`tr`标签，同理，`<table>`标签可能不存在，所以要用`isinstance()`函数判断
  + 删除`find_all()`函数返回的列表中的第一个元素，即股票名称，保留剩余信息
+ 列表中储存的是`tr`标签，共有4个，每个`tr`标签包含两个`td`标签，`td`标签包含股票信息。所以对每个`tr`标签，需要用`find_aLL()`函数找出里面的`td`标签，存储于`data`列表中
  + 对每个`td`标签里的文本信息，通过`split()`函数分开冒号前后的部分，取出第一个部分存为键，第二部分存为值。
+ 这里需要加一个库，来消除`\xa0`和`u3000`字符，这里`\xa0`表示不间断空白符。有几种办法消除，参考`https://www.jianshu.com/p/56d4babcc555`的办法，使用`unicodedata`模块，来消除这些空白符。
  + 通过`dict[key]=value`来向字典中添加元素

7. 用`open()`函数写入文件
8. 整个BeautifulSoup分析过程都需要放在`try...except`框架中，防止程序报错退出
9. 使用`traceback()`函数，可以找到报错的原因，便于排查

****

## 主函数

```python
def main():
    stock_list_url = 'http://quote.eastmoney.com/stock_list.html'
    stock_info_url = 'http://so.cfi.cn/so.aspx?txquery='
    output_file = 'E://Notes/WebSpider/03 第三周 网络爬虫之实战/单元9：实例3：股票数据定向爬虫/BaiduStockInfo.txt'

```

`output_file`路径开头要双斜杠，结尾必须加上文件名和后缀，不然无法储存



****

# “股票数据定向爬虫”实例优化

提高用户体验：速度

使用Requests-BeautifulSoup技术路线，速度不会提升很快，如果想要进一步提速，需要使用Scrapy库。



## 速度提高：编码识别的优化

```python
def getHTMLText(url):
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        print('爬取失败')
        return ""
```

在获取HTML页面的代码中，有一行：` r.encoding = r.apparent_encoding`，其中`r.apparent_encoding`是由程序来判断，其中的文本可能使用了什么编码方式，而`r.encoding`只是从HTML的头文件中去解析它可能用到的编码方式，所以`r.apparent_encoding`步骤需要很多时间。

对于定向爬取，尤其是爬取同样的网页，可以人工判断网页是什么编码，直接赋给`r.encoding`就可以了。这样可以节省时间。

例子：

```python
def getHTMLText(url, code='utf-8'):
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        r.encoding = code
        return r.text
    except:
        print('爬取失败')
        return ""
```

对于东方财富网，它使用的编码是`GB2312`编码，我们可以直接把这个编码赋值给函数

```python
def getStockList(lst, stockURL):
    html = getHTMLText(stockURL, 'GB2312')
    soup = BeautifulSoup(html, 'html.parser')
    a = soup.find_all('a')
    for i in a:
        try:
            href = i.attrs['href']
            lst.append(re.findall(r'[s][hz]\d{6}', href)[0])
        except:
            continue
```

由于用于查找股票详情的网站--中财搜索使用的是`utf-8`编码的，所以无需更改属性。



## 体验提高：增加动态进度显示

打印进度百分比

修改`getStockInfo()`函数：

```python
def getStockInfo(lst, stockURL, fpath):
    count = 0
    for stock in lst:
        url = stockURL + stock
        print(url)
        html = getHTMLText(url)
        try:
            if html == "":
                continue
            infoDict = {}
            soup = BeautifulSoup(html, 'html.parser')
            stockInfo = soup.find('table', attrs={'class': 'quote'})
            if isinstance(stockInfo, bs4.element.Tag):
                name = stockInfo.find('a', attrs={'target': '_blank'}).parent
            else:
                continue
            name_text = name.text.split()[0]
            infoDict.update({'股票名称': name_text})
            if isinstance(stockInfo, bs4.element.Tag):
                keyList = stockInfo.find_all('tr')
            else:
                continue
            del keyList[0]

            for i in range(len(keyList)):
                data = keyList[i].find_all('td')
                key1 = unicodedata.normalize('NFKC', data[0].text.split(':')[0])
                val1 = data[0].text.split(':')[1]
                key2 = unicodedata.normalize('NFKC', data[1].text.split(':')[0])
                val2 = data[1].text.split(':')[1]
                infoDict[key1] = val1
                infoDict[key2] = val2
            with open(fpath, 'a', encoding='utf-8') as f:
                f.write(str(infoDict) + '\n')
                count = count + 1
                print('\r当前速度: {:/2f}%'.format(count*100/len(lst)), end='')
        except:
            count = count + 1
            print('\r当前速度: {:/2f}%'.format(count*100/len(lst)), end='')
            traceback.print_exc()
            continue
```

增加一个变量`count`，以股票列表的长度作为基数，当前的`count`作为分子部分，这样获得比例。

实现不换行：使用转义符`\r`，`\r`可以将打印的字符串的最后的光标提到当前这一行的头部，下一次进行打印的时候，打印的信息就会覆盖之前打印的内容，`.2f`保留两位小数，另外在`print()`函数末尾加上`end=''`防止自动换行，这样就可以实现打印变化的小数百分比了。



****

# 单元小结

采用`requests-bs4-re`路线实现了股票信息爬取和存储

注意：

1. 在选取信息来源的时候，注意网页使用的编写代码，是不是用JavaScript动态显示数据，还是用HTML静态地保存了数据的内容。采用这条路线，我们只能爬取HTML中存在的数据内容。
2. 在进行页面内容的数据检索中，要学会合理地使用`BeautifulSoup库`与`re库`，并能将两者结合获取内容。对于那些非常有特征的数据，我们可以用正则表达式获取。如果是某些数据存在的区域相对比较固定，我们可以通过`BeautifulSoup库`定位到这个标签上，然后再通过正则表达式获取其中的内容。
3. 实现优化，展示了爬取过程的动态小数百分比显示。

好的程序与差的程序体现在用户体验上。